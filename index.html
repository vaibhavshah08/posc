#Genetic Algorithm

import numpy as np
import random

def calculate_fitness(route, distance_matrix):
    total_distance = 0
    for i in range(len(route) - 1):
        total_distance += distance_matrix[route[i]][route[i + 1]]
    total_distance += distance_matrix[route[-1]][route[0]]
    return total_distance

def generate_parent():
    parent = random.sample(range(10), 10)
    return parent

def crossover_and_mutation(parent1, parent2):
    crossover_point = random.randint(1, len(parent1) - 1)
    child1 = parent1[:crossover_point] + parent2[crossover_point:]
    child2 = parent2[:crossover_point] + parent1[crossover_point:]

    for i in range(crossover_point):
        if child1[i] not in child2:
            continue
        else:
            for j in range(crossover_point, len(child2)):
                if child2[j] not in child1:
                    child1[i], child2[j] = child2[j], child1[i]
                    break

    mutation_rate = 0.1
    for i in range(len(child1)):
        if random.random() < mutation_rate:
            j = random.randint(0, len(child1) - 1)
            child1[i], child2[j] = child2[j], child1[i]

    return child1, child2

def main():
    distance_matrix = np.random.randint(50, 100, (10, 10))
    np.fill_diagonal(distance_matrix, 0)

    print("Distance Matrix:")
    print(distance_matrix)

    parents = [generate_parent() for _ in range(10)]

    parent_fitness = [calculate_fitness(parent, distance_matrix) for parent in parents]

    print("\nInitial Parents:")
    for i, (parent, fitness) in enumerate(zip(parents, parent_fitness)):
        print(f"Parent {i+1}: {parent} | Fitness: {fitness}")

    least_fitness_children = []

    for iteration in range(10):
        children = []
        for i in range(0, len(parents), 2):
            parent1, parent2 = parents[i], parents[i + 1]
            child1, child2 = crossover_and_mutation(parent1, parent2)
            children.append(child1)
            children.append(child2)

        print(f"\nIteration {iteration + 1}:")
        for child in children:
            child_fitness = calculate_fitness(child, distance_matrix)
            print(f"Child: {child} | Fitness: {child_fitness}")

        least_fitness_child = min(children, key=lambda x: calculate_fitness(x, distance_matrix))
        least_fitness_children.append(least_fitness_child)

        parents = children

    overall_least_fitness_child = min(least_fitness_children, key=lambda x: calculate_fitness(x, distance_matrix))
    overall_least_fitness = calculate_fitness(overall_least_fitness_child, distance_matrix)

    print(f"\nOverall Least Fitness Child: {overall_least_fitness_child} | Fitness: {overall_least_fitness}")

if __name__ == "__main__":
    main()

---------------------------------------------------------

# McCulloch-Pitts model for OR gate
def mcculloch_pitts_or_gate(x1, x2):
    if x1 + x2 > 0:
        return 1
    else:
        return 0

# McCulloch-Pitts model for XOR gate
def mcculloch_pitts_xor_gate(x1, x2):
    if x1 + x2 == 1:
        return 1
    else:
        return 0

# Testing the OR gate
print(mcculloch_pitts_or_gate(0, 0))  # Output: 0
print(mcculloch_pitts_or_gate(0, 1))  # Output: 1
print(mcculloch_pitts_or_gate(1, 0))  # Output: 1
print(mcculloch_pitts_or_gate(1, 1))  # Output: 1

# Testing the XOR gate
print(mcculloch_pitts_xor_gate(0, 0))  # Output: 0
print(mcculloch_pitts_xor_gate(0, 1))  # Output: 1
print(mcculloch_pitts_xor_gate(1, 0))  # Output: 1
print(mcculloch_pitts_xor_gate(1, 1))  # Output: 0

--------------------------------------------------------------

# Madaline XOR and AND

import numpy as np

# Define the XOR truth table (bipolar)
# X = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1]])
# Y = np.array([-1, 1, 1, -1])
# and gate binary inputs
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
Y = np.array([0, 0, 0, 1])
# or gate binary inputs
# X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
# Y = np.array([0, 1, 1, 1])
# Initialize weights and bias with small random values
W_A = np.random.rand()
W_B = np.random.rand()
bias_weight = np.random.rand()
learning_rate = 0.1
epochs = 100

# Training the MADALINE network
for epoch in range(epochs):
    errors = 0
    print(f"Epoch {epoch + 1}:")

    for i in range(len(X)):
        A, B = X[i]
        target = Y[i]

        # Calculate the weighted sum
        weighted_sum = A * W_A + B * W_B + bias_weight

        # Compute the network output using bipolar step function
        output = 1 if weighted_sum > 0 else 0

        # Update weights and bias using the perceptron learning rule
        delta_W_A = learning_rate * (target - output) * A
        delta_W_B = learning_rate * (target - output) * B
        delta_bias_weight = learning_rate * (target - output)

        W_A += delta_W_A
        W_B += delta_W_B
        bias_weight += delta_bias_weight

        errors += int(target != output)

        print(f"  Input: ({A}, {B}), Target: {target}, Output: {output}")
        print(f"  Weights: W_A = {W_A:.4f}, W_B = {W_B:.4f}, bias_weight = {bias_weight:.4f}")
        print(f"  Updated Weights: delta_W_A = {delta_W_A:.4f}, delta_W_B = {delta_W_B:.4f}, delta_bias_weight = {delta_bias_weight:.4f}\n")

    # Calculate and print the error for the current epoch
    error_rate = errors / len(X)
    print(f"Epoch {epoch + 1} Error Rate: {error_rate * 100:.2f}%")

    # Check if all training patterns are classified correctly
    if errors == 0:
        print(f"Converged after {epoch + 1} epochs.")
        break

# Test the trained MADALINE network
print("Testing the trained MADALINE network:")
for i in range(len(X)):
    A, B = X[i]
    target = Y[i]

    weighted_sum = A * W_A + B * W_B + bias_weight
    output = 1 if weighted_sum >= 0 else -1

    print(f"Input: ({A}, {B}), Target: {target}, Output: {output}")

----------------------------------------------------------------------------------------

# Hebb network
import numpy as np
import matplotlib.pyplot as plt

input = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])


def step_function(sum):
    if sum >= 1:
        return 1
    return 0


def output(w, instance, bias):
    sum = instance.dot(w) + bias
    return step_function(sum)



def hebb(outputs, w, bias):
    for i in range(4):

        w[0] = w[0] + (input[i][0] * outputs[i])
        w[1] = w[1] + (input[i][1] * outputs[i])
        bias = bias + (1 * outputs[i])

        print("Weight updated: " + str(w[0]))
        print("Weight updated: " + str(w[1]))
        print("Bias updated: " + str(bias))


    return w, bias
and_outputs = np.array([0, 1, 1, 1])
w = np.array([0.0, 0.0])
bias = 0
mae = 1

returned_w, returned_bias = hebb(and_outputs, w, bias)
r_and_outputs = np.array([output(returned_w, np.array([[1, 1]]), returned_bias),output(returned_w, np.array([[1, -1]]), returned_bias),output(returned_w, np.array([[-1, 1]]), returned_bias),output(returned_w, np.array([[-1, -1]]), returned_bias)])

print(r_and_outputs)
def mean_absolute_error(actual_values, predicted_values):
    if len(actual_values) != len(predicted_values):
        raise ValueError("Input lists must have the same length.")

    absolute_errors = [abs(actual - predicted) for actual, predicted in zip(actual_values, predicted_values)]
    mean_error = sum(absolute_errors) / len(actual_values)

    return mean_error

mae = mean_absolute_error(and_outputs, r_and_outputs)
print("Mean Absolute Error:", mae)


--------------------------------------------------------------------------


#Bi-polar sigmoid
import numpy as np

# Define the sigmoid activation function and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Define the neural network class
class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # Initialize the weights and biases
        self.weights_input_hidden = np.random.rand(self.input_size, self.hidden_size)
        self.bias_hidden = np.random.rand(1, self.hidden_size)
        self.weights_hidden_output = np.random.rand(self.hidden_size, self.output_size)
        self.bias_output = np.random.rand(1, self.output_size)

    def forward_propagation(self, X):
        # Perform the forward propagation step
        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden
        self.hidden_output = sigmoid(self.hidden_input)
        self.output = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output
        final_output = sigmoid(self.output)
        return final_output

    def backward_propagation(self, X, y, output, learning_rate):
        # Perform the backward propagation step
        error = y - output
        d_output = error * sigmoid_derivative(output)
        error_hidden = d_output.dot(self.weights_hidden_output.T)
        d_hidden = error_hidden * sigmoid_derivative(self.hidden_output)

        # Update the weights and biases
        self.weights_hidden_output += self.hidden_output.T.dot(d_output) * learning_rate
        self.bias_output += np.sum(d_output, axis=0, keepdims=True) * learning_rate
        self.weights_input_hidden += X.T.dot(d_hidden) * learning_rate
        self.bias_hidden += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate

    def train(self, X, y, epochs, learning_rate):
        # Train the neural network
        for epoch in range(epochs):
            output = self.forward_propagation(X)
            self.backward_propagation(X, y, output, learning_rate)

# Example usage
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# Create a neural network with 2 input neurons, 2 hidden neurons, and 1 output neuron
nn = NeuralNetwork(2, 2, 1)

# Train the neural network
nn.train(X, y, epochs=10000, learning_rate=0.1)

# Test the trained model
output = nn.forward_propagation(X)
print(output)

------------------------------------------------------------------------------------

#CNN

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
(train_images, train_labels), (test_images, test_labels) = datasets.mnist.
↪load_data()
train_images, test_images = train_images / 255.0, test_images / 255.0
model = models.Sequential()
model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.AveragePooling2D((2, 2)))
model.add(layers.Conv2D(32, (5, 5), activation='relu'))
model.add(layers.AveragePooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))

model.compile(optimizer='adam',

loss='sparse_categorical_crossentropy',
metrics=['accuracy'])

history = model.fit(train_images.reshape(-1, 28, 28, 1), train_labels, epochs=5,
validation_data=(test_images.reshape(-1, 28, 28, 1),␣

↪test_labels))

test_loss, test_acc = model.evaluate(test_images.reshape(-1, 28, 28, 1),␣
↪test_labels)
print(f'Test accuracy: {test_acc * 100:.2f}%')
predictions = model.predict(test_images.reshape(-1, 28, 28, 1))
import random
def show_random_images_with_predictions(images, labels, predictions,␣
↪num_images=5):
plt.figure(figsize=(10, 5))
for i in range(num_images):
index = random.randint(0, len(images) - 1)
plt.subplot(1, num_images, i + 1)
plt.xticks([])
plt.yticks([])
plt.grid(False)
plt.imshow(images[index], cmap=plt.cm.binary)
predicted_label = tf.argmax(predictions[index])
true_label = labels[index]
plt.xlabel(f"True: {true_label}, Predicted: {predicted_label}")
show_random_images_with_predictions(test_images, test_labels, predictions,␣
↪num_images=5)
plt.show()
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()


-------------------------------------------------------------------------------------


#ART
import numpy as np
learning_rate = 2
vigilance_parameter = 0.4
input_vectors = [[0, 0, 0, 1], [0, 1, 0, 1], [0, 0, 1, 1], [1, 0, 0, 0]]
categories = [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]
num_inputs = len(input_vectors)
initial_activation = 1 / (num_inputs + 1)
weights = []
num_categories = len(categories)
def calculate_product(s, t, j):
product = []
p = 0
for i in t[j]:
product.append(i * s[p])
p = p + 1
return product
for i in range(num_inputs):
category_weights = []
for j in range(num_categories):
category_weights.append(initial_activation)
weights.append(category_weights)
for input_vector in input_vectors:
input_sum = sum(input_vector)
x = input_vector
y = np.dot(input_vector, weights)
y = list(y)
r = 1
while r == 1:
j = y.index(max(y))
x = calculate_product(input_vector, categories, j)
x = list(x)
if (sum(x) / sum(input_vector)) < vigilance_parameter:
y[j] = -1
else:
r = 0
k = 0
if j >= num_categories:
num_categories += 1
categories.append(x)
weights.append([initial_activation] * num_categories) # Append new category weights
else:
for i in range(len(weights[j])):
weights[j][i] = (learning_rate * x[k]) / (learning_rate - 1 + sum(x))
k = k + 1
print(j)
print('weights:')
print(np.array(weights))

-----------------------------------------------------------------------------------------

#Fuzzy

A = {"1": 0.1, "2": 0.2, "3": 0.3, "4": 0.4, "5": 0.5}
B = {"1": 0.3, "2": 0.4, "3": 0.5, "4": 0.6, "5": 0.7}
C = {"3": 0.2, "4": 0.4, "5": 0.6, "6": 0.8, "7": 1.0}
print('First Fuzzy Set:', A)
print('Second Fuzzy Set:', B)
print('Third Fuzzy Set:', C)
def union(set1, set2):
result = {}
for key in set1.keys() | set2.keys():
value1 = set1.get(key, 0)
value2 = set2.get(key, 0)
result[key] = value1 if value1 > value2 else value2
return result
def intersection(set1, set2):
result = {}
for key in set1.keys() & set2.keys():
value1 = set1.get(key, 0)
value2 = set2.get(key, 0)
result[key] = value1 if value1 < value2 else value2
return result
def complement(set1):

A_complement = {key: 1 - value for key, value in A.items()}
B_complement = {key: 1 - value for key, value in B.items()}
C_complement = {key: 1 - value for key, value in C.items()}
print('Complement of A:', A_complement)
print('Complement of B:', B_complement)
print('Complement of C:', C_complement)
lhs = union(A, intersection(B, C))
rhs = intersection(union(A, B), union(A, C))
print('LHS:', lhs)
print('RHS:', rhs)
print('Distributive law:', lhs == rhs)
de_morgans_law_lhs = complement(intersection(A, B))
de_morgans_law_rhs = union(complement(A), complement(B))
print("de morgan law",de_morgans_law_lhs == de_morgans_law_rhs)
lhs = union(A, B)
rhs = union(B, A)
print('LHS:', lhs)
print('RHS:', rhs)
print('Commutative law:', lhs == rhs)
a_lhs = union(A, union(B, C))
a_rhs = union(union(A, B), C)
de_morgans_law_lhs = complement(intersection(A, B))
complement = {}
for key, value in set1.items():
complement[key] = 1 - value
return complement

print('LHS:', a_lhs)
print('RHS:', a_rhs)
print('Associative law:', a_lhs == a_rhs)

----------------------------------------------------------------------------

#linear
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
data = pd.read_csv('R.csv')
x = data['Dry bulb temperature']
y = data['Load']
dry_bulb_temperature = data['Dry bulb temperature']
mean_load = np.mean(y)
n = 99
print(data.head())
B0_num = (y.sum() * (x**2).sum()) - ((x.sum()* (x*y).sum())) 
B0_den = (n * (x**2).sum()) - ((x.sum())**2)
B0 = B0_num/B0_den

print(B0)
B1_num = (x.sum()*y.sum() - (n * (x*y).sum())) 
B1_den = ((x.sum())**2) - (n * (x**2).sum())
B1 = B1_num/B1_den
print(B1)
predicted_values = [B0 + B1 * x for x in dry_bulb_temperature]

mse = np.mean(np.abs((y - predicted_values) / y)) * 100
print(mse)

-------------------------------------------------------------------------------

#PSO

#PSO
import numpy as np
import random

# Initialize distance matrix
distance_matrix = np.random.randint(50, 100, (10, 10))
np.fill_diagonal(distance_matrix, 0)

print("Distance Matrix:")
print(distance_matrix)

# Calculate fitness function for a given route
def calculate_fitness(route, distance_matrix):
    total_distance = 8
    for i in range(len(route) - 1):
        total_distance += distance_matrix[route[i]][route[i + 1]]
    total_distance += distance_matrix[route[-1]][route[0]]
    return total_distance

# Particle class for Particle Swarm Optimization
class Particle:
    def __init__(self, num_cities):
        self.position = random.sample(range(num_cities), num_cities)
        self.velocity = [random.random() for _ in range(len(self.position))]
        self.personal_best_position = self.position
        self.personal_best_fitness = calculate_fitness(self.position, distance_matrix)

# Generate initial particles
particles = [Particle(10) for _ in range(10)]

# Calculate fitness for initial particles
particle_fitness = [calculate_fitness(particle.position, distance_matrix) for particle in particles]

print("\nInitial Particles:")
for i, (particle, fitness) in enumerate(zip(particles, particle_fitness)):
    print(f"Particle {i+1}: {particle.position} | Fitness: {fitness}")

# Perform iterations
for iteration in range(10):
    global_best_position = min(particles, key=lambda x: calculate_fitness(x.position, distance_matrix)).position

    for particle in particles:
        inertia_weight = 1
        social_weight = 2
        cognitive_weight = 2

        particle_velocity = (inertia_weight * np.array(particle.velocity)) + \
                            (social_weight * random.random() * (np.array(global_best_position) - np.array(particle.position))) + \
                            (cognitive_weight * random.random() * (np.array(particle.personal_best_position) - np.array(particle.position)))

        # Apply boundary conditions
        particle.position = np.round(np.maximum(np.minimum(particle.position + particle_velocity, 9), 0)).astype(int)

        # Update personal best
        particle_fitness = calculate_fitness(particle.position, distance_matrix)
        if particle_fitness < particle.personal_best_fitness:
            particle.personal_best_position = particle.position
            particle.personal_best_fitness = particle_fitness

    print(f"\nIteration {iteration + 1}:")
    for particle in particles:
        particle_fitness = calculate_fitness(particle.position, distance_matrix)
        print(f"Particle: {particle.position} | Fitness: {particle_fitness}")

# Select the best particle as the solution
best_particle = min(particles, key=lambda x: calculate_fitness(x.position, distance_matrix))

print("\nBest Solution:")
print(f"Particle: {best_particle.position} | Fitness: {best_particle.personal_best_fitness}")


-----------------------------------------------------------------------------------------------------

#Fuzzy Fast


fuzzy_set_Fast = {0: 0,10: 0.01,20: 0.02,30: 0.05,40: 0.1,50: 0.4,60: 0.8,70: 0.9,80: 1}
fuzzy_set_Dangerous= {0: 0, 10: 0.05,20: 0.1,30: 0.15,40: 0.2,50: 0.3,60: 0.7,70: 1,80: 1}
result=[]
result.append([0,0.05,0.1,0.15,0.2,0.3,0.7,1,1])
for i in range(1,9):
  l=[]
  for j in range(9):
    if fuzzy_set_Dangerous[j*10]>=fuzzy_set_Fast[i*10]:
      l.append(fuzzy_set_Dangerous[j*10])
    else:
      l.append(fuzzy_set_Fast[i*10])
  result.append(l)
print("OR LOGIC")
print(result)
print("AND LOGIC")
result2=[]
for i in range(1,9):
  l=[]
  for j in range(9):
    if fuzzy_set_Dangerous[j*10]<=fuzzy_set_Fast[i*10]:
      l.append(fuzzy_set_Dangerous[j*10])
    else:
      l.append(fuzzy_set_Fast[i*10])
  result2.append(l)
print(result2)


def classical_union(A, B):
    union_result = A.copy()
    for element in B:
        if element not in union_result:
            union_result.add(element)
    return union_result

def classical_intersection(A, B):
    intersection_result = set()
    for element in A:
        if element in B:
            intersection_result.add(element)
    return intersection_result

def fuzzy_union(A, B):
    union_result = {}
    for element in A.keys():
        union_result[element] = A[element]
    for element in B.keys():
        if element in union_result:
          if union_result[element]<B[element]:
            union_result[element] = B[element]
        else:
            union_result[element] = B[element]
    return union_result

def fuzzy_intersection(A, B):
    intersection_result = {}
    for element in A.keys():
        if element in B:
          if A[element]>B[element]:
            intersection_result[element] = A[element]
          else:
            intersection_result[element] = B[element]
    return intersection_result

def fuzzy_complement(A):
    complement_result = {}
    for element, membership in A.items():
        complement_result[element] = (1 - membership)
    return complement_result


classical_set_A = {1, 2, 3, 4, 5}
classical_set_B = { 3, 4, 5, 6, 7, 8, 9}

fuzzy_set_A = {1: 0.8, 2: 0.6, 3: 0.3, 4: 0.9, 5: 0.7}
fuzzy_set_B = {3: 0.6, 4: 0.5, 5: 0.6, 6: 0.4, 7: 0.2, 8: 0.1, 9: 0.9}

classical_union_result = classical_union(classical_set_A, classical_set_B)
classical_intersection_result = classical_intersection(classical_set_A, classical_set_B)

fuzzy_union_result = fuzzy_union(fuzzy_set_A, fuzzy_set_B)
fuzzy_intersection_result = fuzzy_intersection(fuzzy_set_A, fuzzy_set_B)
fuzzy_complement_A = fuzzy_complement(fuzzy_set_A)
fuzzy_complement_B = fuzzy_complement(fuzzy_set_B)

print("Classical Union (A U B):", classical_union_result)
print("Classical Intersection (A ∩ B):", classical_intersection_result)
print("Fuzzy Union (A U B):", fuzzy_union_result)
print("Fuzzy Intersection (A ∩ B):", fuzzy_intersection_result)
print("Fuzzy Complement of A (~A):", fuzzy_complement_A)
print("Fuzzy Complement of B (~B):", fuzzy_complement_B)


-----------------------------------------------------------------------------------------

#perceptron

import numpy as np

# Define the activation function (step function)
def step_function(x):
    return 1 if x >= 0 else 0

# Define the XOR gate inputs and outputs
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 1, 1, 0])

# Define the weights for the first layer
w1 = np.array([[1, 1], [1, 1]])

# Define the weights for the second layer
w2 = np.array([1, -2])

# Define the bias for the first layer
b1 = np.array([-1, -1])

# Define the bias for the second layer
b2 = np.array([-0.5])

# Forward propagation
def forward_propagation(x):
    a1 = np.dot(x, w1) + b1
    h1 = np.array([step_function(x) for x in a1])
    a2 = np.dot(h1, w2) + b2
    output = step_function(a2)
    return output

# Test the model
for i in range(len(X)):
    output = forward_propagation(X[i])
    print("Input:", X[i], "Output:", output)


## fulladder

# Define the full adder inputs and outputs
X = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],
              [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])
y = np.array([0, 1, 1, 0, 1, 0, 0, 1])

# Define the weights for the first layer
w1 = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])

# Define the weights for the second layer
w2 = np.array([1, 1, 1])

# Define the bias for the first layer
b1 = np.array([-2, -1, -1])

# Define the bias for the second layer
b2 = np.array([-1.5])

# Forward propagation
def forward_propagation(x):
    a1 = np.dot(x, w1) + b1
    h1 = np.array([step_function(x) for x in a1])
    a2 = np.dot(h1, w2) + b2
    output = step_function(a2)
    return output

# Test the model
for i in range(len(X)):
    output = forward_propagation(X[i])
    print("Input:", X[i], "Output:", output)


#####
import numpy as np
import matplotlib.pyplot as plt
class Perceptron:
def __init__(self, input_size):
self.input_size = input_size
self.bias = np.random.uniform(-1, 1)
def predict(self, inputs, weights):
ws = np.dot(inputs, weights) + self.bias
return 1 if ws >= 0 else 0
def train(self, training_data, labels, epochs, learning_rate):
errors = []
for _ in range(epochs):
self.weights = np.random.uniform(-1, 1, self.input_size)
total_error = 0
for inputs, label in zip(training_data, labels):
prediction = self.predict(inputs, self.weights)
error = label - prediction
if error != 0:
self.weights += learning_rate * error * inputs
self.weights = np.clip(self.weights, -1, 1)

total_error += abs(error)
errors.append(total_error / len(training_data))
print(f"Weights at epoch {_+1}: {self.weights}")
return errors

training_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
labels = np.array([0, 1, 1, 0])

perceptron = Perceptron(input_size=2)

epochs = 100
learning_rate = 0.1
error_history = perceptron.train(training_data, labels, epochs,
learning_rate)

plt.plot(range(epochs), error_history)
plt.xlabel('Epochs')
plt.ylabel('Mean Absolute Error')
plt.title('Mean Absolute Error During Training')
plt.grid(True)
plt.show()

test_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
for inputs in test_inputs:
prediction = perceptron.predict(inputs, perceptron.weights)
print(f"Inputs: {inputs} Prediction: {prediction}")

Code for Full Adder:
import numpy as np
import matplotlib.pyplot as plt
class FullAdder:
def __init__(self, input_size):
self.input_size = input_size
self.bias = np.random.uniform(-1, 1)
def predict(self, inputs, weights):
ws = np.dot(inputs, weights) + self.bias
return 1 if ws >= 0 else 0
def train(self, training_data, labels, epochs, learning_rate):
errors = []
for _ in range(epochs):
self.weights = np.random.uniform(-1, 1, self.input_size)
total_error = 0
for inputs, label in zip(training_data, labels):
prediction = self.predict(inputs, self.weights)
error = label - prediction

if error != 0:
self.weights += learning_rate * error * inputs
self.weights = np.clip(self.weights, -1, 1)

total_error += abs(error)
errors.append(total_error / len(training_data))
print(f"Weights at epoch {_+1}: {self.weights}")
return errors

training_data = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],
[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])
labels = np.array([0, 1, 1, 0, 1, 0, 0, 1])

full_adder = FullAdder(input_size=3)

epochs = 100
learning_rate = 0.1
error_history = full_adder.train(training_data, labels, epochs,
learning_rate)

plt.plot(range(epochs), error_history)
plt.xlabel('Epochs')
plt.ylabel('Mean Absolute Error')
plt.title('Mean Absolute Error During Training')
plt.show()

test_inputs = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1,
0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])
for inputs in test_inputs:
prediction = full_adder.predict(inputs, full_adder.weights)
print(f"Inputs: {inputs} Prediction: {prediction}")
